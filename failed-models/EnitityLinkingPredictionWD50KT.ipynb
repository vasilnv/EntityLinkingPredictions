{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1255f759",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cf189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.datasets import WD50KT\n",
    "\n",
    "dataset = WD50KT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa81ad",
   "metadata": {},
   "source": [
    "## Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('labels.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c0f09",
   "metadata": {},
   "source": [
    "## Get embeddings for entity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "my_literal_information = []\n",
    "\n",
    "batch_size = 50\n",
    "for i in range(0, len(labels), batch_size):\n",
    "    batch_labels = labels[i:i+batch_size]\n",
    "\n",
    "    # Tokenize labels and convert to input IDs\n",
    "    inputs = tokenizer(batch_labels, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Take the mean of the embeddings to get a single vector for each label\n",
    "    batch_literal_information = embeddings.mean(dim=1)\n",
    "\n",
    "    my_literal_information.append(batch_literal_information)\n",
    "\n",
    "# Concatenate all batches\n",
    "my_literal_information = torch.cat(my_literal_information)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56f2e4",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "\n",
    "result = pipeline(\n",
    "    dataset=dataset,\n",
    "    model='ComplExLiteral',\n",
    "    training_kwargs=dict(num_epochs=250),\n",
    "    evaluation_relation_whitelist={\"P138\"}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc638b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.save_to_directory('doctests/test_unstratified_complex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933228e",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pykeen.models import Model\n",
    "from pykeen import predict\n",
    "\n",
    "from pykeen.datasets import get_dataset\n",
    "from pykeen.predict import predict_target\n",
    "\n",
    "# Get the trained model\n",
    "model = result.model\n",
    "\n",
    "# Create a mapping for relations\n",
    "relation_label_to_id = result.training.relation_to_id\n",
    "\n",
    "# Find the relation id for \"named after\"\n",
    "named_after_id = relation_label_to_id[\"P138\"]\n",
    "\n",
    "# use conferences relations for the Nations dataset, remove this when used for named after and Wikidata\n",
    "# named_after_id = relation_label_to_id[\"conferences\"]\n",
    "\n",
    "# Get the relation label\n",
    "named_after_label = result.training.relation_id_to_label[named_after_id]\n",
    "\n",
    "# Prepare a dictionary to store predictions\n",
    "predictions = {}\n",
    "\n",
    "# Loop over all entities\n",
    "numOfAllEntities = model.num_entities\n",
    "i = 1\n",
    "for head_id, head_label in result.training.entity_id_to_label.items():\n",
    "    # Use `predict_target` to predict the tail entity\n",
    "    prediction_df = predict.predict_target(\n",
    "        model=model,\n",
    "        head=head_label,\n",
    "        relation=named_after_label,\n",
    "        triples_factory=result.training,\n",
    "    ).df\n",
    "\n",
    "    id = head_label\n",
    "\n",
    "    # Create parameters\n",
    "    params = {\n",
    "                'action': 'wbgetentities',\n",
    "                'ids':id,\n",
    "                'format': 'json',\n",
    "                'languages': 'en'\n",
    "            }\n",
    "\n",
    "    # fetch the API\n",
    "    data = fetch_wikidata(params)\n",
    "\n",
    "    # Show response\n",
    "    data = data.json()\n",
    "    head_id_label = data['entities'][id]['labels']['en']['value']\n",
    "\n",
    "    f = open(\"predicted.txt\", \"a\")\n",
    "    line = \"Predictions for \" + head_label + \" with label \" + head_id_label + \":\\n\"\n",
    "    f.write(line)\n",
    "    f.close()\n",
    "\n",
    "    predicted = prediction_df.head(5)\n",
    "    for index, row in predicted.iterrows():\n",
    "        score = row['score']\n",
    "        tail = row['tail_label']\n",
    "        params = {\n",
    "              'action': 'wbgetentities',\n",
    "              'ids':tail,\n",
    "              'format': 'json',\n",
    "              'languages': 'en'\n",
    "          }\n",
    "        tailData = fetch_wikidata(params)\n",
    "\n",
    "        data = tailData.json()\n",
    "        tail_id_label = data['entities'][tail]['labels']['en']['value']\n",
    "        f = open(\"gosho.csv\", \"a\")\n",
    "        line = \"Predicted tail \" + tail + \" with score \" + str(score) + \" with tail label \" + tail_id_label + \"\\n\"\n",
    "        f.write(line)\n",
    "        f.close()\n",
    "\n",
    "    f = open(\"gosho.csv\", \"a\")\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.close()\n",
    "\n",
    "    # Store the prediction data frame\n",
    "    predictions[head_label] = prediction_df.head(5)\n",
    "    if (i % 100) == 0:\n",
    "        print(f'Generated predictions for entity {i} out of {numOfAllEntities}')\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
